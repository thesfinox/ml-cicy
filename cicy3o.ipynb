{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Complete Intersection Calabi-Yau Manifold\n",
    "\n",
    "In the framework of String Theory, we apply machine learning (ML) techniques for the prediction of the Hodge numbers of Complete Intersection Calabi-Yau (CICY) 3-folds. The relevant quantities are therefore $h_{11}$ and $h_{21}$ which can be predicted starting from the configuration matrices of known manifolds.\n",
    "\n",
    "We will use both unsupervised and supervised algorithms to produce an engineered dataset to improve the prediction abilities of different algorithms. We use libraries such as [_Scikit-learn_](https://scikit-learn.org/stable/) and [_XGBoost_](https://xgboost.readthedocs.io/en/latest/) in order to fit the data and [_Scikit-optimize_](https://scikit-optimize.github.io/stable/) for hyperparameter optimization. For the neural network analysis we use [Tensorflow](https://www.tensorflow.org/) and its _Keras_ module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib        as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import sklearn           as skl\n",
    "import skopt             as sko\n",
    "import tensorflow        as tf\n",
    "import xgboost           as xgb\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "print('Python version: {:d}.{:d}'.format(sys.version_info.major, sys.version_info.minor))\n",
    "print('Matplot version: {}'.format(mpl.__version__))\n",
    "print('Numpy version: {}'.format(np.__version__))\n",
    "print('Pandas version: {}'.format(pd.__version__))\n",
    "print('Scikit-learn version: {}'.format(skl.__version__))\n",
    "print('Scikit-optimize version: {}'.format(sko.__version__))\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "print('Keras version: {} (backend: {})'.format(keras.__version__, K.backend()))\n",
    "print('XGBoost version: {}'.format(xgb.__version__))\n",
    "\n",
    "# fix random_seed\n",
    "RAND = 42\n",
    "np.random.seed(RAND)\n",
    "tf.random.set_seed(RAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardware specifications are as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OS info:')\n",
    "!uname -o\n",
    "!lsb_release -d | sed 's/^.*:\\s*//g'\n",
    "\n",
    "print('\\n\\nCPU info:')\n",
    "!lscpu | grep 'Model name' | sed 's/^.*:\\s*//g'\n",
    "\n",
    "print('\\n\\nGPU info:')\n",
    "!lspci | grep '3D controller' | sed 's/^.*controller:\\s*//g'\n",
    "!nvidia-smi\n",
    "\n",
    "print('\\n\\nRAM info:')\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a file logger to store debug information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os import path, rename\n",
    "from time import strftime, gmtime\n",
    "\n",
    "def create_logfile(filename, name='logger', level=logging.INFO):\n",
    "    \"\"\"Create a logfile and rotate old logs.\n",
    "    \n",
    "    Required arguments:\n",
    "        filename: the name of the file or path to the log\n",
    "        \n",
    "    Optional arguments\n",
    "        name:     the name of the log session\n",
    "        level:    the level of the information stores\n",
    "        \n",
    "    Returns:\n",
    "        the log\n",
    "    \"\"\"\n",
    "    \n",
    "    # get current time to rename strings\n",
    "    ctime = strftime('.%Y%m%d.%H%M%S', gmtime())\n",
    "    \n",
    "    # rotate log if it already exists\n",
    "    if path.isfile(filename):\n",
    "        rename(filename, filename + ctime)\n",
    "    \n",
    "    # get a logging session by name\n",
    "    log = logging.getLogger(name + ctime)\n",
    "    log.setLevel(level)\n",
    "    \n",
    "    # define format\n",
    "    fmt = logging.Formatter('%(asctime)s --> %(levelname)s: %(message)s')\n",
    "    \n",
    "    # add the log file\n",
    "    han = logging.FileHandler(filename=filename)\n",
    "    han.setLevel(level)\n",
    "    han.setFormatter(fmt)\n",
    "    \n",
    "    # add handler for standard output\n",
    "    std = logging.StreamHandler(sys.stdout)\n",
    "    std.setLevel(level)\n",
    "    std.setFormatter(fmt)\n",
    "    \n",
    "    # create the output\n",
    "    log.addHandler(han)\n",
    "    log.addHandler(std)\n",
    "    \n",
    "    return log\n",
    "\n",
    "def logprint(string, stream='info', logger=None):\n",
    "    \"\"\"Decides whether to print on the logger or the standard output.\n",
    "    \n",
    "    Required arguments:\n",
    "        string: the string to print\n",
    "    \n",
    "    Optional arguments:\n",
    "        stream: standard input (info) or standard error (error)\n",
    "        logger: the logger (None for standard output/error)\n",
    "    \"\"\"\n",
    "    \n",
    "    if logger is not None:\n",
    "        if stream == 'info':\n",
    "            logger.info(string)\n",
    "        elif stream == 'error':\n",
    "            logger.error(string)\n",
    "        else:\n",
    "            logger.debug(string)\n",
    "    else:\n",
    "        if stream == 'info':\n",
    "            sys.stdout.write(string)\n",
    "        elif stream == 'error':\n",
    "            sys.stderr.write(string)\n",
    "        else:\n",
    "            sys.stdout.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Tools\n",
    "\n",
    "We first fetch the desired dataset and prepare the tools for the analysis. Specifically we need to:\n",
    "\n",
    "1. define the names of the main directories and create them if non existent,\n",
    "2. import the database,\n",
    "3. create tools for visualisation and manipulation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from the directories and the name of the dataset to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os import makedirs\n",
    "\n",
    "ROOT_DIR = '.'      # root directory\n",
    "IMG_DIR  = 'img'    # image directory\n",
    "MOD_DIR  = 'models' # directory of saved models\n",
    "LOG_DIR  = 'log'    # directory for logs\n",
    "\n",
    "# name of the dataset to be considered\n",
    "DB_NAME = 'cicy3o'\n",
    "DB_FILE = DB_NAME + '.h5'              # full name with extension\n",
    "DB_PATH = path.join(ROOT_DIR, DB_FILE) # full path\n",
    "DB_DIR  = 'original' if DB_NAME == 'cicy3o' else 'favourable' # subdir where to store images, models, logs\n",
    "\n",
    "# define full paths\n",
    "IMG_PATH = path.join(ROOT_DIR, IMG_DIR, DB_DIR)\n",
    "MOD_PATH = path.join(ROOT_DIR, MOD_DIR, DB_DIR)\n",
    "LOG_PATH = path.join(ROOT_DIR, LOG_DIR, DB_DIR)\n",
    "\n",
    "# create directories if non existent\n",
    "if not path.isdir(IMG_PATH):\n",
    "    makedirs(IMG_PATH, exist_ok=True)\n",
    "if not path.isdir(MOD_PATH):\n",
    "    makedirs(MOD_PATH, exist_ok=True)\n",
    "if not path.isdir(LOG_PATH):\n",
    "    makedirs(LOG_PATH, exist_ok=True)\n",
    "    \n",
    "# create logfile\n",
    "logger = create_logfile(filename=path.join(LOG_PATH, DB_NAME + '.log'), name='CICY3', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "from urllib import request as rq\n",
    "\n",
    "URL_ROOT = 'http://www.lpthe.jussieu.fr/~erbin/files/data/'\n",
    "TAR_FILE = DB_NAME + '_data.tar.gz'\n",
    "TAR_PATH = path.join(ROOT_DIR, TAR_FILE)\n",
    "\n",
    "if not path.isfile(TAR_PATH): # fetch file only if not already downloaded\n",
    "    logprint('Fetching dataset...\\n', logger=logger)\n",
    "    _, message = rq.urlretrieve(URL_ROOT + TAR_FILE, TAR_PATH)\n",
    "    logger.info('Dataset fetched!')\n",
    "    \n",
    "if path.isfile(TAR_PATH):\n",
    "    logprint('Extracting dataset from tarball...', logger=logger)\n",
    "    with tarfile.open(TAR_PATH, 'r') as tar:\n",
    "        tar.extract(DB_FILE, path=ROOT_DIR)\n",
    "    logprint('Dataset extracted!', logger=logger)\n",
    "else:\n",
    "    logprint('Tarball non available: cannot extract tarball!', stream='error', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then prepare to load the dataset and prepare for the visualisation analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(filepath, mode='hdf5', shuffle=False, random_state=None, logger=None):\n",
    "    \"\"\"Load a dataset given the path and the format.\n",
    "    \n",
    "    Required arguments:\n",
    "        filepath: the path of the file\n",
    "        \n",
    "    Optional arguments:\n",
    "        mode:         the format of the file\n",
    "        shuffle:      whether to shuffle the file\n",
    "        random_state: the seed of the random generator\n",
    "        logger:       the logging session (None for standard output)\n",
    "        \n",
    "    Returns:\n",
    "        the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if path.isfile(filepath):\n",
    "        logprint('Reading database...', logger=logger)\n",
    "        if mode == 'hdf5':\n",
    "            df = pd.read_hdf(filepath)\n",
    "        elif mode == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "        logprint('Database loaded!', logger=logger)\n",
    "    else:\n",
    "        logprint('Database is not available: cannot load the database!', stream='error', logger=logger)\n",
    "        \n",
    "    # shuffle the dataframe\n",
    "    if shuffle:\n",
    "        df = skl.utils.shuffle(df, random_state=random_state)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define some functions we can use to extract and manipulate the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# remove the outliers from a Pandas dataset\n",
    "class RemoveOutliers(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Remove outlying data given a dataset and a dictionary containing the intervals for each class.\n",
    "    \n",
    "    E.g.: if the two classes are 'h11' and 'h21', the dictionary will be: {'h11': [1, 16], 'h21': [1, 86]}.\n",
    "    \n",
    "    Public methods:\n",
    "        fit:           unused method\n",
    "        transform:     remove data outside the given interval\n",
    "        fit_transform: equivalent to transform(fit(...))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filter_dict=None):\n",
    "        \"\"\"Constructor of the class.\n",
    "        \n",
    "        Optional arguments:\n",
    "            filter_dict: the intervals to retain in the data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.filter_dict = filter_dict\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Unused method.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform the input by deleting data outside the interval\n",
    "        \n",
    "        Required arguments:\n",
    "            X: the dataset\n",
    "            \n",
    "        Returns:\n",
    "            the transformed dataset\n",
    "        \"\"\"\n",
    "\n",
    "        x = X.copy() # avoid overwriting\n",
    "\n",
    "        if self.filter_dict is not None:\n",
    "            for key in self.filter_dict:\n",
    "                x = x.loc[x[key] >= self.filter_dict[key][0]]\n",
    "                x = x.loc[x[key] <= self.filter_dict[key][1]]\n",
    "\n",
    "        return x\n",
    "\n",
    "# extract the tensors from a Pandas dataset\n",
    "class ExtractTensor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract a dense tensor from sparse input from a given dataset.\n",
    "    \n",
    "    Public methods:\n",
    "        fit:           unused method\n",
    "        transform:     extract dense tensor\n",
    "        fit_transform: equivalent to transform(fit(...))\n",
    "        get_shape:     compute the shape of the tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, flatten=False, shape=None):\n",
    "        \"\"\"Constructor of the class.\n",
    "        \n",
    "        Optional arguments:\n",
    "            flatten: whether to flatten the output or keep the current shape\n",
    "            shape:   force the computation with a given shape\n",
    "        \"\"\"\n",
    "\n",
    "        self.flatten = flatten\n",
    "        self.shape   = shape\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Unused method.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute the dense equivalent of the sparse input.\n",
    "        \n",
    "        Required arguments:\n",
    "            X: the dataset\n",
    "            \n",
    "        Returns:\n",
    "            the transformed input\n",
    "        \"\"\"\n",
    "\n",
    "        x = X.copy() # avoid overwriting\n",
    "        if self.shape is None:\n",
    "            self.shape = x.apply(np.shape).max() # get the shape of the tensor\n",
    "\n",
    "        if len(self.shape) > 0: # apply this to vectors and tensors\n",
    "            offset = lambda s : [ (0, self.shape[i] - np.shape(s)[i]) for i in range(len(self.shape)) ]\n",
    "            x      = x.apply(lambda s: np.pad(s, offset(s), mode='constant'))\n",
    "\n",
    "        if self.flatten and len(self.shape) > 0:\n",
    "            return list(np.stack(x.apply(np.ndarray.flatten).values))\n",
    "        else:\n",
    "            return list(np.stack(x.values))\n",
    "\n",
    "    def get_shape(self):\n",
    "        \"\"\"Compute the shape of the tensor.\n",
    "        \n",
    "        Returns:\n",
    "            the shape of the tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.shape\n",
    "\n",
    "# get the accuracy (possibly after rounding)\n",
    "def accuracy_score(y_true, y_pred, rounding=np.rint):\n",
    "    \"\"\"Compute the accuracy of the predictions after rounding.\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values\n",
    "        y_pred: predicted values\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    assert np.shape(y_true)[0] == np.shape(y_pred)[0] # check if same length\n",
    "    \n",
    "    # if same length then proceed\n",
    "    accuracy = 0\n",
    "    if rounding is not None:\n",
    "        for n in range(np.shape(y_true)[0]):\n",
    "            accuracy = accuracy + 1 \\\n",
    "                       if int(y_true[n]) == int(rounding(y_pred[n])) \\\n",
    "                       else accuracy\n",
    "    else:\n",
    "        for n in range(np.shape(y_true)[0]):\n",
    "            accuracy = accuracy + 1 \\\n",
    "                       if y_true[n] == y_pred[n] \\\n",
    "                       else accuracy\n",
    "    return accuracy / np.shape(y_true)[0]\n",
    "\n",
    "# get the error difference (possibly after rounding)\n",
    "def error_diff(y_true, y_pred, rounding=np.rint):\n",
    "    \"\"\"Compute the error difference between true values and predictions (positive values are overestimate and viceversa).\n",
    "    \n",
    "    Required arguments:\n",
    "        y_true: true values\n",
    "        y_pred: predicted values\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    assert np.shape(y_true)[0] == np.shape(y_pred)[0] # check if same length\n",
    "\n",
    "    # if same length then proceed\n",
    "    err = y_true - rounding(y_pred)\n",
    "    return np.array(err).astype(np.int8)\n",
    "\n",
    "# print *SearchCV scores\n",
    "def gridcv_score(estimator, rounding=np.rint, logger=None):\n",
    "    \"\"\"Print scores given by cross-validation and optimisation techniques.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator: the estimator to be evaluated\n",
    "        \n",
    "    Optional arguments:\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "        logger:   the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "    \n",
    "    best_params = estimator.best_params_              # get best parameters\n",
    "    df          = pd.DataFrame(estimator.cv_results_) # dataframe with CV res.\n",
    "    \n",
    "    cv_best_res = df.loc[df['params'] == best_params] # get best results\n",
    "    accuracy    = cv_best_res.loc[:, 'mean_test_score'].values[0]\n",
    "    std         = cv_best_res.loc[:, 'std_test_score'].values[0]\n",
    "    \n",
    "    logprint('Best parameters: {}'.format(best_params), logger=logger)\n",
    "    logprint('Accuracy ({}) of cross-validation: ({:.3f} Â± {:.3f})%'.format(rounding.__name__, accuracy*100, std*100), logger=logger)\n",
    "    \n",
    "# print the accuracy of the predictions\n",
    "def prediction_score(estimator, X, y, use_best_estimator=False, rounding=np.rint, logger=None):\n",
    "    \"\"\"Print the accuracy of the predictions.\n",
    "    \n",
    "    Required arguments:\n",
    "        estimator: the estimator to be used for the predictions\n",
    "        X: the features\n",
    "        y: the labels (actual values)\n",
    "        \n",
    "    Optional arguments:\n",
    "        use_best_estimator: whether to use the estimator.best_estimator_ or just estimator\n",
    "        rounding: the Numpy function for rounding the predictions\n",
    "        logger:   the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_best_estimator:\n",
    "        estimator = estimator.best_estimator_\n",
    "    \n",
    "    accuracy = accuracy_score(y, estimator.predict(X), rounding=rounding)\n",
    "    logprint('Accuracy ({}) of the predictions: {:.3f}%'.format(rounding.__name__, accuracy*100), logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use _Matplotlib_ to plot the data. We define a few functions which we can use during the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set label sizes\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=12)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# set building block sizes for the plot\n",
    "mpl_width  = 6\n",
    "mpl_height = 5\n",
    "\n",
    "# save the current figure\n",
    "def save_fig(filename, tight_layout=True, extension='png', resolution=96, logger=None):\n",
    "    \"\"\"Save current figure to file.\n",
    "    \n",
    "    Required arguments:\n",
    "        filename: the name of the file where to save the figure (without extension)\n",
    "        \n",
    "    Optional arguments:\n",
    "        tight_layout: whether to use the tight_layout\n",
    "        extension:    extension of the file to use\n",
    "        resolution:   resolution of the file\n",
    "        logger:       the logging session (None for standard output)\n",
    "    \"\"\"\n",
    "\n",
    "    filename = path.join(IMG_PATH, filename + '.' + extension)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "\n",
    "    logprint('Saving {}...'.format(filename), logger=logger)\n",
    "    plt.savefig(filename, format=extension, dpi=resolution)\n",
    "    logprint('Saved {}!'.format(filename), logger=logger)\n",
    "\n",
    "# get a generator to count the occurrencies\n",
    "def get_counts(df, label, feature):\n",
    "    \"\"\"Generator to produce the count of unique occurrencies of the data.\n",
    "    \n",
    "    Required arguments:\n",
    "        df:      the Pandas dataframe\n",
    "        label:   the label to consider\n",
    "        feature: the feature to consider\n",
    "        \n",
    "    Yields:\n",
    "        [ unique feature, unique value, counts ]\n",
    "    \"\"\"\n",
    "\n",
    "    for n in np.sort(df[feature].unique()):\n",
    "        uniques, counts = np.unique(df[label].loc[df[feature] == n].values, return_counts=True)\n",
    "        for u, c in np.c_[uniques, counts]:\n",
    "            yield [ n, u, c ]\n",
    "\n",
    "# plot histogram of occurrencies\n",
    "def count_plot(ax, data, title=None, xlabel=None, ylabel='N',\n",
    "               legend=None, xlog=False, ylog=False, binstep=5,\n",
    "               **kwargs):\n",
    "    \"\"\"Plot histogram of occurrencies (e.g.: frequency plot).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        binstep:  the distance between adjacent bins\n",
    "        **kwargs: additional arguments to pass to plt.hist\n",
    "    \"\"\"\n",
    "\n",
    "    min_tick = np.min(data) if np.min(data) > -100 else -100 # set a MIN cut\n",
    "    max_tick = np.max(data) if np.max(data) < 100  else 100  # set a MAX cut\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create a grid\n",
    "    ax.set_title(title)                  # set title\n",
    "    ax.set_xlabel(xlabel)                # set a label for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set a label for the y axis\n",
    "    ax.set_xticks(np.arange(min_tick,    # set no. of ticks in the x axis\n",
    "                            max_tick,\n",
    "                            step=binstep\n",
    "                           )\n",
    "                 )\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    ax.hist(data,                        # create histogram using 'step' funct.\n",
    "            histtype='step',\n",
    "            label=legend,\n",
    "            **kwargs)\n",
    "\n",
    "    if legend is not None:               # add legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot labeled features and their values\n",
    "def label_plot(ax, data, title=None, xlabel=None, ylabel='values',\n",
    "               legend=None, xlog=False, ylog=False, binstep=1,\n",
    "               **kwargs):\n",
    "    \"\"\"Plot values of labelled data (e.g.: variable ranking).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        binstep:  the distance between adjacent bins\n",
    "        **kwargs: additional arguments to pass to plt.plot\n",
    "    \"\"\"\n",
    "\n",
    "    labels      = [f[0] for f in data]   # labels vector\n",
    "    importances = [f[1] for f in data]   # importances vector\n",
    "    length      = len(labels)            # length of the labels vector\n",
    "    \n",
    "    ax.grid(alpha=0.2)                   # create a grid\n",
    "    ax.set_title(title)                  # set title\n",
    "    ax.set_xlabel(xlabel)                # set a label for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set a label for the x axis\n",
    "\n",
    "    ax.set_xticks(np.arange(length,      # set no. of ticks in the x axis\n",
    "                            step=binstep\n",
    "                           )\n",
    "                 )\n",
    "    ax.set_xticklabels(labels,           # set name of labels of the x axis\n",
    "                       ha='right',       # horizontal alignment\n",
    "                       rotation=45       # rotation of the labels\n",
    "                      )\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    ax.plot(np.arange(length),           # plot data\n",
    "            importances,\n",
    "            label=legend,\n",
    "            **kwargs)\n",
    "\n",
    "    if legend is not None:               # add legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot the correlation matrix of a Pandas dataframe\n",
    "def mat_plot(ax, df, label='correlation matrix', **kwargs):\n",
    "    \"\"\"Plot the correlation matrix of a given dataframe.\n",
    "    \n",
    "    Required arguments:\n",
    "        ax: the subplot ax where to plot data\n",
    "        df: the dataframe\n",
    "        \n",
    "    Optional arguments:\n",
    "        label:    the label to use for the colour bar\n",
    "        **kwargs: additional arguments to pass to plt.matshow\n",
    "    \"\"\"\n",
    "\n",
    "    matrix = df.corr()                   # create correlation matrix\n",
    "    labels = df.columns.tolist()         # extract the name of the labels\n",
    "\n",
    "    ax.set_xticks(np.arange(len(labels), # set ticks for x axis\n",
    "                  step=1)\n",
    "                 )\n",
    "    ax.set_xticklabels([''] + labels,    # set the name of the ticks\n",
    "                       rotation=90\n",
    "                      )\n",
    "\n",
    "    ax.set_yticks(np.arange(len(labels), # set ticks for y axis\n",
    "                  step=1)\n",
    "                 )\n",
    "    ax.set_yticklabels([''] + labels)    # set the name of the ticks\n",
    "                    \n",
    "    matshow = ax.matshow(matrix,         # show the matrix\n",
    "                         vmin=-1.0,\n",
    "                         vmax=1.0,\n",
    "                         **kwargs\n",
    "                        )\n",
    "                                \n",
    "    cbar = ax.figure.colorbar(matshow,   # create the colour bar\n",
    "                              ax=ax,\n",
    "                              fraction=0.05,\n",
    "                              pad=0.05\n",
    "                             )\n",
    "    cbar.ax.set_ylabel(label,            # show the colour bar\n",
    "                       va='bottom',      # vertical alignment\n",
    "                       rotation=-90)     # rotation of the label\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot a scatter plot with colours and sizes\n",
    "def scatter_plot(ax, data, title=None, xlabel=None, ylabel=None,\n",
    "                 legend=None, xlog=False, ylog=False,\n",
    "                 colour=True, size=True, colour_label='N', size_leg=0,\n",
    "                 **kwargs):\n",
    "    \"\"\"Scatter plot of occurrencies with colour and size codes.\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:        the title of the plot\n",
    "        xlabel:       the label of the x axis\n",
    "        ylabel:       the label of the y axis\n",
    "        legend:       the label for the legend in the plot\n",
    "        xlog:         whether to use the log scale on the x axis\n",
    "        ylog:         whether to use the log scale on the y axis\n",
    "        colour:       whether to use colour codes\n",
    "        size:         whether to use entries of different size\n",
    "        colour_label: label to use for the colour code\n",
    "        size_leg:     length of the legend of the size code\n",
    "        **kwargs:     additional arguments to pass to plt.scatter\n",
    "    \"\"\"\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create  a grid\n",
    "    ax.set_xlabel(xlabel)                # set labels for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set labels for the y axis\n",
    "    ax.set_title(title)                  # set title\n",
    "\n",
    "    if xlog:                             # use log scale in x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    if colour:                           # create the plot with size and colours\n",
    "        if size:\n",
    "            scat = ax.scatter(data[0], data[1], s=data[2], c=data[2], **kwargs)\n",
    "        else:\n",
    "            scat = ax.scatter(data[0], data[1], c=data[2], **kwargs)\n",
    "        cbar = ax.figure.colorbar(scat, ax=ax)\n",
    "        cbar.ax.set_ylabel(colour_label, rotation=-90, va='bottom')\n",
    "    else:\n",
    "        if size:\n",
    "            scat = ax.scatter(data[0], data[1], s=data[2], **kwargs)\n",
    "        else:\n",
    "            scat = ax.scatter(data[0], data[1], **kwargs)\n",
    "\n",
    "    scat.set_label(legend)               # set label of the plot\n",
    "    if size_leg:                         # add the size legend if needed\n",
    "        handles, labels = scat.legend_elements('sizes', num=size_leg)\n",
    "        ax.legend(handles, labels, loc='lower center',\n",
    "                  bbox_to_anchor=(0.5,-0.3), ncol=len(handles),\n",
    "                  fontsize='medium', frameon=False)\n",
    "\n",
    "    if legend:                           # show the legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# plot a series with trivial x label\n",
    "def series_plot(ax, data, title=None, xlabel='series', ylabel=None,\n",
    "                legend=None, xlog=False, ylog=False,\n",
    "                step=False, std=False,\n",
    "                **kwargs):\n",
    "    \"\"\"Plot a series of data with ordered x axis (e.g.: epoch series).\n",
    "    \n",
    "    Required arguments:\n",
    "        ax:   the subplot ax where to plot data\n",
    "        data: the data to plot\n",
    "        \n",
    "    Optional arguments:\n",
    "        title:    the title of the plot\n",
    "        xlabel:   the label of the x axis\n",
    "        ylabel:   the label of the y axis\n",
    "        legend:   the label for the legend in the plot\n",
    "        xlog:     whether to use the log scale on the x axis\n",
    "        ylog:     whether to use the log scale on the y axis\n",
    "        step:     whether to use a step function for the plot\n",
    "        std:      highlight the strip of the standard deviation\n",
    "        **kwargs: additional arguments to pass to plt.step or plot.plot\n",
    "    \"\"\"\n",
    "\n",
    "    ax.grid(alpha=0.2)                   # create the grid\n",
    "    ax.set_title(title)                  # set the title\n",
    "    ax.set_xlabel(xlabel)                # set labels for the x axis\n",
    "    ax.set_ylabel(ylabel)                # set labels for the y axis\n",
    "\n",
    "    if xlog:                             # use log scale in the x axis if needed\n",
    "        ax.set_xscale('log')\n",
    "    if ylog:                             # use log scale in the y axis if needed\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    series = np.arange(len(data))        # create trivial x axis data\n",
    "    if step:                             # create the plot\n",
    "        ax.step(series, data, label=legend, **kwargs)\n",
    "    else:\n",
    "        ax.plot(series, data, label=legend, **kwargs)\n",
    "\n",
    "    if std:                              # show coloured strip with std\n",
    "        ax.fill_between(series,\n",
    "                        data + np.std(data),\n",
    "                        data - np.std(data),\n",
    "                        alpha=0.2)\n",
    "\n",
    "    if legend is not None:               # show the legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further we also set the **memory growth** of the GPU RAM in order to avoid memory issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of installed GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True) # set memory growth\n",
    "            \n",
    "        # get the list of logical devices (GPUs)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        logprint('GPU setup: {:d} physical GPUs, {:d} logical GPUs.'.format(len(gpus), len(logical_gpus)), logger=logger)\n",
    "    except RuntimeError as e:\n",
    "        logprint(e, stream='error', logger=logger)\n",
    "else:\n",
    "    logprint('No GPUs in the setup!', stream='error', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation\n",
    "\n",
    "We first visualise the data inside the dataframe. We focus on manifolds which are not direct products of other spaces (i.e. we consider only entries with `isprod = 0`) and consider $h_{11} \\in [ 1, 16 ]$ and $h_{21} \\in [ 1, 86 ]$. We show the distribution of the labels both in their frequency and with respect to a few of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database\n",
    "df = load_dataset(DB_PATH, shuffle=True, random_state=RAND, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reference we print the name of the columns and their respective _dtypes_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then extract only the entries such that `isprod = 0` and remove the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove product spaces\n",
    "logprint('Removing product spaces...', logger=logger)\n",
    "df_noprod = df.loc[df['isprod'] == 0].drop(columns='isprod')\n",
    "logprint('Product spaces removed!', logger=logger)\n",
    "\n",
    "# remove outliers\n",
    "filter_dict = {'h11': [1,16], 'h21': [1,86]}\n",
    "logprint('Removing outliers...', logger=logger)\n",
    "df_noprod_noout = RemoveOutliers(filter_dict=filter_dict).fit_transform(df_noprod)\n",
    "logprint('Outliers removed!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurrencies of the Labels\n",
    "\n",
    "We then plot the distributions for $h_{11}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplots  = 2\n",
    "yplots  = 2\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax[0,0], df['h11'],              title='Distribution of $h_{11}$ (original dataset)',                xlabel='$h_{11}$', ylog=True)\n",
    "count_plot(ax[0,1], df_noprod['h11'],       title='Distribution of $h_{11}$ (w/o product spaces)',              xlabel='$h_{11}$', ylog=True)\n",
    "count_plot(ax[1,0], df_noprod_noout['h11'], title='Distribution of $h_{11}$ (w/o product spaces and outliers)', xlabel='$h_{11}$', ylog=True)\n",
    "\n",
    "count_plot(ax[1,1], df['h11'],              title='Distribution of $h_{11}$', xlabel='$h_{11}$', ylog=True, legend='original')\n",
    "count_plot(ax[1,1], df_noprod['h11'],       title='Distribution of $h_{11}$', xlabel='$h_{11}$', ylog=True, legend='no prod')\n",
    "count_plot(ax[1,1], df_noprod_noout['h11'], title='Distribution of $h_{11}$', xlabel='$h_{11}$', ylog=True, legend='no out')\n",
    "\n",
    "save_fig('h11_distribution', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the distributions for $h_{21}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplots  = 2\n",
    "yplots  = 2\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax[0,0], df['h11'],              title='Distribution of $h_{21}$ (original dataset)',                xlabel='$h_{21}$', ylog=True)\n",
    "count_plot(ax[0,1], df_noprod['h21'],       title='Distribution of $h_{21}$ (w/o product spaces)',              xlabel='$h_{21}$', ylog=True)\n",
    "count_plot(ax[1,0], df_noprod_noout['h21'], title='Distribution of $h_{21}$ (w/o product spaces and outliers)', xlabel='$h_{21}$', ylog=True)\n",
    "\n",
    "count_plot(ax[1,1], df['h21'],              title='Distribution of $h_{21}$', xlabel='$h_{21}$', ylog=True, legend='original')\n",
    "count_plot(ax[1,1], df_noprod['h21'],       title='Distribution of $h_{21}$', xlabel='$h_{21}$', ylog=True, legend='no prod')\n",
    "count_plot(ax[1,1], df_noprod_noout['h21'], title='Distribution of $h_{21}$', xlabel='$h_{21}$', ylog=True, legend='no out')\n",
    "\n",
    "save_fig('h21_distribution', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the Labels\n",
    "\n",
    "Now consider a few scalar features and show the distribution of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat_features = [ 'num_cp', 'num_eqs', 'norm_matrix', 'rank_matrix' ]\n",
    "scat_labels   = [ 'h11', 'h21' ]\n",
    "\n",
    "fig, ax = plt.subplots(len(scat_features), len(scat_labels), figsize=(len(scat_labels)*mpl_width, len(scat_features)*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "for n in range(len(scat_features)):\n",
    "    for m in range(len(scat_labels)):\n",
    "        scatter_plot(ax[n,m], np.array(list(get_counts(df_noprod_noout, scat_labels[m], scat_features[n]))).T,\n",
    "                     title='Distribution of the labels', xlabel=scat_features[n], ylabel=scat_labels[m],\n",
    "                     colour_label='no. of occurrencies', size_leg=5)\n",
    "        \n",
    "save_fig('h11_h21_distribution', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Unsupervised Preprocessing\n",
    "\n",
    "We then extract usable data and start to analyse the features through unsupervised preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only the dataframe without product spaces and outliers (and remove irrelevant features)\n",
    "df = df_noprod_noout.drop(labels=df_noprod_noout.filter(regex='min|max|mean|media|c2|redun|size|euler|favour'), axis=1)\n",
    "\n",
    "# divide features and labels\n",
    "labels  = ['h11', 'h21']\n",
    "logprint('Selecting labels...', logger=logger)\n",
    "df_labs = df[labels]\n",
    "logprint('Labels selected!', logger=logger)\n",
    "df_feat = df.drop(labels=labels, axis=1)\n",
    "\n",
    "# the only tensor feature\n",
    "tensor_feat = ['matrix']\n",
    "\n",
    "# then extract the others\n",
    "logprint('Selecting features...', logger=logger)\n",
    "scalar_feat = list(df_feat.select_dtypes(include=[np.int8, np.int16, np.int64, np.float16, np.float32, np.float64, np.float128]).columns)\n",
    "vector_feat = list(df_feat.drop(labels=tensor_feat, axis=1).select_dtypes(include=['object']).columns)\n",
    "logprint('Features selected!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to extract the features using the `ExtractTensor` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in vector_feat:\n",
    "    logprint('Extracting {} from vector features...'.format(feature), logger=logger)\n",
    "    df_feat[feature] = ExtractTensor(flatten=False).fit_transform(df_feat[feature]) # do not flatten the output\n",
    "logprint('Vector features have been extracted!', logger=logger)\n",
    "\n",
    "for feature in tensor_feat:\n",
    "    logprint('Extracting {} from tensor features...'.format(feature), logger=logger)\n",
    "    df_feat[feature] = ExtractTensor(flatten=False).fit_transform(df_feat[feature]) # do not flatten the output\n",
    "logprint('Tensor features have been extracted!', logger=logger)\n",
    "\n",
    "df_feat = df_feat[scalar_feat + vector_feat + tensor_feat]\n",
    "logprint('Features have been fully extracted!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then show the correlation matrix between the scalar features in order to better understand the relation between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplots  = 3\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "mat_plot(ax[0], df_labs)\n",
    "mat_plot(ax[1], df_feat[scalar_feat])\n",
    "mat_plot(ax[2], df_labs.join(df_feat)[['h11', 'h21', 'num_cp', 'num_eqs', 'num_over', 'num_ex', 'rank_matrix', 'norm_matrix']])\n",
    "\n",
    "save_fig('correlation_matrix', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbour Clustering\n",
    "\n",
    "We consider the `KMeans` clustering of the component of the configuration matrix to probe the distribution of its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# flatten the matrix\n",
    "flat_matrix       = ExtractTensor(flatten=True).fit_transform(df_feat['matrix'])\n",
    "flat_matrix_shape = np.shape(flat_matrix)\n",
    "\n",
    "# create an empty array to store the labels\n",
    "cluster_range = range(2,20)\n",
    "kmeans_labels = np.empty((flat_matrix_shape[0],cluster_range.stop - cluster_range.start), dtype=np.int8)\n",
    "\n",
    "# compute various clustering classifications\n",
    "for n_clusters in cluster_range:\n",
    "    logprint('Computing clustering for {:d} clusters...'.format(n_clusters), logger=logger)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RAND, n_jobs=-1)\n",
    "    kmeans.fit_transform(flat_matrix)\n",
    "    kmeans_labels[:,n_clusters - cluster_range.start] = kmeans.labels_\n",
    "logprint('Clustering task ended!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then include this into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering to dataframe\n",
    "df_feat['clustering'] = list(kmeans_labels)\n",
    "logprint('Adding clustering labels to dataframe...', logger=logger)\n",
    "\n",
    "# reorder the dataframe\n",
    "df_feat = df_feat[scalar_feat + ['clustering'] + vector_feat + tensor_feat]\n",
    "logprint('Labels have been added!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis\n",
    "\n",
    "We then proceed with the `PCA` on the configuration matrix. We compute the algorithms with 2 principal components only for plotting purposes but we will the discard the results. Instead we will keep the 99% variance PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# compute the PCA\n",
    "logprint('Computing PCA with 2 components...', logger=logger)\n",
    "pca_2 = PCA(n_components=2, random_state=RAND)\n",
    "matrix_pca_2 = pca_2.fit_transform(flat_matrix)\n",
    "logprint('PCA with 2 components computed!', logger=logger)\n",
    "logprint('Ratio of the variance retained for each component: {:.2f}%, {:.2f}%'.format(pca_2.explained_variance_ratio_[0]*100, pca_2.explained_variance_ratio_[1]*100), logger=logger)\n",
    "\n",
    "# plot the labels with respect to the principal components\n",
    "xplots  = 2\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "scatter_plot(ax[0], [matrix_pca_2[:,0], matrix_pca_2[:,1], df_labs['h11']], title='Distribution of $h_{11}$ w.r.t. PCA(n = 2)', xlabel='PCA #1', ylabel='PCA #2', size=True)\n",
    "scatter_plot(ax[1], [matrix_pca_2[:,0], matrix_pca_2[:,1], df_labs['h21']], title='Distribution of $h_{21}$ w.r.t. PCA(n = 2)', xlabel='PCA #1', ylabel='PCA #2', size=True)\n",
    "\n",
    "save_fig('h11_h21_pca_2_comp', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute the \"good\" `PCA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the PCA with 99% variance\n",
    "logprint('Computing PCA with 99% variance...', logger=logger)\n",
    "matrix_pca99 = PCA(n_components=0.99, random_state=42).fit_transform(flat_matrix)\n",
    "logprint('PCA with 99% variance computed!', logger=logger)\n",
    "\n",
    "# analyse the result\n",
    "matrix_pca99_shape = matrix_pca99.shape\n",
    "logprint('No. of components of the PCA: {:d}'.format(matrix_pca99_shape[1]), logger=logger)\n",
    "\n",
    "# save the results inside the dataframe\n",
    "df_feat['matrix_pca99'] = list(matrix_pca99)\n",
    "logprint('Adding PCA to dataframe...', logger=logger)\n",
    "\n",
    "# reorder the dataframe\n",
    "df_feat = df_feat[scalar_feat + ['clustering'] + vector_feat + tensor_feat + ['matrix_pca99']]\n",
    "logprint('PCA has been added!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Ranking and Feature Selection\n",
    "\n",
    "Using the engineered features including `PCA` and `KMeans`, we then train a [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) in order to extract the variables ranking for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector of shapes inside the dataframe in order to be able to extract the features\n",
    "features_shapes = {}\n",
    "\n",
    "# scalars\n",
    "for feature in scalar_feat:\n",
    "    features_shapes[feature] = 1\n",
    "\n",
    "# clustering\n",
    "features_shapes['clustering']   = df_feat['clustering'].apply(np.shape).unique()[0][0] # unique() returns an array, thus we need to take only one element (the first)\n",
    "\n",
    "rnd_for_features = np.c_[df_feat[scalar_feat].values,\n",
    "                         ExtractTensor(flatten=True).fit_transform(df_feat['clustering']),\n",
    "                        ] # create features input\n",
    "\n",
    "# vectors\n",
    "for feature in vector_feat:\n",
    "    features_shapes[feature] = df_feat[feature].apply(np.shape).unique()[0][0]\n",
    "    rnd_for_features = np.c_[rnd_for_features, ExtractTensor(flatten=True).fit_transform(df_feat[feature])]\n",
    "\n",
    "# tensors\n",
    "for feature in tensor_feat:\n",
    "    features_shapes[feature] = df_feat[feature].apply(np.shape).unique()[0][0] * df_feat[feature].apply(np.shape).unique()[0][1]\n",
    "    rnd_for_features = np.c_[rnd_for_features, ExtractTensor(flatten=True).fit_transform(df_feat[feature])]\n",
    "\n",
    "# pca   \n",
    "features_shapes['matrix_pca99'] = df_feat['matrix_pca99'].apply(np.shape).unique()[0][0]\n",
    "rnd_for_features = np.c_[rnd_for_features, ExtractTensor(flatten=True).fit_transform(df_feat['matrix_pca99'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed with the decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd_for_param = {'n_estimators': 100,\n",
    "                 'n_jobs':       -1,\n",
    "                 'random_state': RAND\n",
    "                }\n",
    "\n",
    "logprint('Computing random forest for h11...', logger=logger)\n",
    "rnd_for_h11 = RandomForestRegressor(**rnd_for_param)\n",
    "rnd_for_h11.fit(rnd_for_features, df_labs['h11'])\n",
    "logprint('Random forest for h11 completed!', logger=logger)\n",
    "logprint('Accuracy of the random forest for h11: {:.3f}%'.format(accuracy_score(df_labs['h11'].values, rnd_for_h11.predict(rnd_for_features), rounding=np.floor)*100), logger=logger)\n",
    "\n",
    "logprint('Computing random forest for h21...', logger=logger)\n",
    "rnd_for_h21 = RandomForestRegressor(**rnd_for_param)\n",
    "rnd_for_h21.fit(rnd_for_features, df_labs['h21'])\n",
    "logprint('Random forest for h21 completed!', logger=logger)\n",
    "logprint('Accuracy of the random forest for h21: {:.3f}%'.format(accuracy_score(df_labs['h21'].values, rnd_for_h21.predict(rnd_for_features), rounding=np.floor)*100), logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then consider the features importance and plot them for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with all the components\n",
    "extended_features = []\n",
    "for feature in features_shapes:\n",
    "    extended_features.append(feature)\n",
    "    for _ in range(features_shapes[feature] - 1):\n",
    "        extended_features.append('')\n",
    "        \n",
    "# list of feature importances\n",
    "feat_imp_h11 = list(zip(extended_features, rnd_for_h11.feature_importances_))\n",
    "feat_imp_h21 = list(zip(extended_features, rnd_for_h21.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the scalar features first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "label_plot(ax, feat_imp_h11[0:np.sum([features_shapes[feature] for feature in scalar_feat])], title='Scalar Features', ylabel='Variable Ranking', legend='$h_{11}$')\n",
    "label_plot(ax, feat_imp_h21[0:np.sum([features_shapes[feature] for feature in scalar_feat])], title='Scalar Features', ylabel='Variable Ranking', legend='$h_{21}$')\n",
    "\n",
    "save_fig('feat_imp_scalars', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then consider vector features and clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplots  = 2\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "label_plot(ax[0], feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])], title='Vector Features (per component)', ylabel='Variable Ranking', legend='$h_{11}$')\n",
    "label_plot(ax[0], feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])], title='Vector Features (per component)', ylabel='Variable Ranking', legend='$h_{21}$')\n",
    "\n",
    "label_plot(ax[1], feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']], title='Cluster Features', ylabel='Variable Ranking', legend='$h_{11}$')\n",
    "label_plot(ax[1], feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']], title='Cluster Features', ylabel='Variable Ranking', legend='$h_{21}$')\n",
    "\n",
    "save_fig('feat_imp_vectors_clustering', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then consider the matrix and `PCA`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplots  = 2\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "label_plot(ax[0], feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                               +features_shapes['matrix']], title='Matrix Features', ylabel='Variable Ranking (per component)', legend='$h_{11}$', binstep=10)\n",
    "label_plot(ax[0], feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                               +features_shapes['matrix']], title='Matrix Features', ylabel='Variable Ranking (per component)', legend='$h_{21}$', binstep=10)\n",
    "\n",
    "label_plot(ax[1], feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                               +features_shapes['matrix']\n",
    "                               :], title='PCA Features', ylabel='Variable Ranking', legend='$h_{11}$', binstep=10)\n",
    "label_plot(ax[1], feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                               +features_shapes['clustering']\n",
    "                               +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                               +features_shapes['matrix']\n",
    "                               :], title='PCA Features', ylabel='Variable Ranking', legend='$h_{21}$', binstep=10)\n",
    "\n",
    "save_fig('feat_imp_matrix_pca', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we can plot the separate sum of each vector and tensor feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of the scalars\n",
    "scalar_h11_sum = np.sum([f[1] for f in feat_imp_h11[0:np.sum([features_shapes[feature] for feature in scalar_feat])]])\n",
    "scalar_h21_sum = np.sum([f[1] for f in feat_imp_h21[0:np.sum([features_shapes[feature] for feature in scalar_feat])]])\n",
    "\n",
    "# sum of dim_cp importances\n",
    "dim_cp_h11_sum = np.sum([f[1] for f in feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           +features_shapes['dim_cp']]])\n",
    "dim_cp_h21_sum = np.sum([f[1] for f in feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           +features_shapes['dim_cp']]])\n",
    "\n",
    "# sum of num_dim_cp importances\n",
    "num_dim_cp_h11_sum = np.sum([f[1] for f in feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               +features_shapes['num_dim_cp']]])\n",
    "num_dim_cp_h21_sum = np.sum([f[1] for f in feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               +features_shapes['num_dim_cp']]])\n",
    "\n",
    "# sum of deg_eqs importances\n",
    "deg_eqs_h11_sum = np.sum([f[1] for f in feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                            +features_shapes['clustering']\n",
    "                                                            +features_shapes['dim_cp']\n",
    "                                                            +features_shapes['num_dim_cp']\n",
    "                                                            :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                            +features_shapes['clustering']\n",
    "                                                            +features_shapes['dim_cp']\n",
    "                                                            +features_shapes['num_dim_cp']\n",
    "                                                            +features_shapes['deg_eqs']]])\n",
    "deg_eqs_h21_sum = np.sum([f[1] for f in feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                            +features_shapes['clustering']\n",
    "                                                            +features_shapes['dim_cp']\n",
    "                                                            +features_shapes['num_dim_cp']\n",
    "                                                            :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                            +features_shapes['clustering']\n",
    "                                                            +features_shapes['dim_cp']\n",
    "                                                            +features_shapes['num_dim_cp']\n",
    "                                                            +features_shapes['deg_eqs']]])\n",
    "\n",
    "# sum of num_deg_eqs importances\n",
    "num_deg_eqs_h11_sum = np.sum([f[1] for f in feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                                +features_shapes['clustering']\n",
    "                                                                +features_shapes['dim_cp']\n",
    "                                                                +features_shapes['num_dim_cp']\n",
    "                                                                +features_shapes['deg_eqs']\n",
    "                                                                :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                                +features_shapes['clustering']\n",
    "                                                                +features_shapes['dim_cp']\n",
    "                                                                +features_shapes['num_dim_cp']\n",
    "                                                                +features_shapes['deg_eqs']\n",
    "                                                                +features_shapes['num_deg_eqs']]])\n",
    "num_deg_eqs_h21_sum = np.sum([f[1] for f in feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                                +features_shapes['clustering']\n",
    "                                                                +features_shapes['dim_cp']\n",
    "                                                                +features_shapes['num_dim_cp']\n",
    "                                                                +features_shapes['deg_eqs']\n",
    "                                                                :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                                +features_shapes['clustering']\n",
    "                                                                +features_shapes['dim_cp']\n",
    "                                                                +features_shapes['num_dim_cp']\n",
    "                                                                +features_shapes['deg_eqs']\n",
    "                                                                +features_shapes['num_deg_eqs']]])\n",
    "\n",
    "# sum of dim_h0_amb importances\n",
    "dim_h0_amb_h11_sum = np.sum([f[1] for f in feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               +features_shapes['num_dim_cp']\n",
    "                                                               +features_shapes['deg_eqs']\n",
    "                                                               +features_shapes['num_deg_eqs']\n",
    "                                                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               +features_shapes['num_dim_cp']\n",
    "                                                               +features_shapes['deg_eqs']\n",
    "                                                               +features_shapes['num_deg_eqs']\n",
    "                                                               +features_shapes['dim_h0_amb']]])\n",
    "dim_h0_amb_h21_sum = np.sum([f[1] for f in feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               +features_shapes['num_dim_cp']\n",
    "                                                               +features_shapes['deg_eqs']\n",
    "                                                               +features_shapes['num_deg_eqs']\n",
    "                                                               :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                               +features_shapes['clustering']\n",
    "                                                               +features_shapes['dim_cp']\n",
    "                                                               +features_shapes['num_dim_cp']\n",
    "                                                               +features_shapes['deg_eqs']\n",
    "                                                               +features_shapes['num_deg_eqs']\n",
    "                                                               +features_shapes['dim_h0_amb']]])\n",
    "\n",
    "# sum of the vector importances\n",
    "vector_h11_sum = dim_cp_h11_sum + num_dim_cp_h11_sum + deg_eqs_h11_sum + num_deg_eqs_h11_sum + dim_h0_amb_h11_sum\n",
    "vector_h21_sum = dim_cp_h21_sum + num_dim_cp_h21_sum + deg_eqs_h21_sum + num_deg_eqs_h21_sum + dim_h0_amb_h21_sum\n",
    "\n",
    "# sum of the importance of the matrix components\n",
    "matrix_h11_sum = np.sum([f[1] for f in feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                                                           :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                                                           +features_shapes['matrix']]])\n",
    "matrix_h21_sum = np.sum([f[1] for f in feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                                                           :np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                           +features_shapes['clustering']\n",
    "                                                           +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                                                           +features_shapes['matrix']]])\n",
    "\n",
    "# sum of the importance of the PCA components\n",
    "pca_h11_sum = np.sum([f[1] for f in feat_imp_h11[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                        +features_shapes['clustering']\n",
    "                                                        +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                                                        +features_shapes['matrix']\n",
    "                                                        :]])\n",
    "pca_h21_sum = np.sum([f[1] for f in feat_imp_h21[np.sum([features_shapes[feature] for feature in scalar_feat])\n",
    "                                                        +features_shapes['clustering']\n",
    "                                                        +np.sum([features_shapes[feature] for feature in vector_feat])\n",
    "                                                        +features_shapes['matrix']\n",
    "                                                        :]])\n",
    "\n",
    "# plot the sum of vector and tensor features\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "label_plot(ax,\n",
    "           [('dim_cp',       dim_cp_h11_sum),\n",
    "            ('num_dim_cp',   num_dim_cp_h11_sum),\n",
    "            ('deg_eqs',      deg_eqs_h11_sum),\n",
    "            ('num_deg_eqs',  num_deg_eqs_h11_sum),\n",
    "            ('dim_h0_amb',   dim_h0_amb_h11_sum),\n",
    "            ('matrix',       matrix_h11_sum),\n",
    "            ('matrix_pca99', pca_h11_sum)\n",
    "           ],\n",
    "           title='Vector and Tensor Features (sum of the components)',\n",
    "           ylabel='Variable Ranking',\n",
    "           legend='$h_{11}$'\n",
    "          )\n",
    "label_plot(ax,\n",
    "           [('dim_cp',       dim_cp_h21_sum),\n",
    "            ('num_dim_cp',   num_dim_cp_h21_sum),\n",
    "            ('deg_eqs',      deg_eqs_h21_sum),\n",
    "            ('num_deg_eqs',  num_deg_eqs_h21_sum),\n",
    "            ('dim_h0_amb',   dim_h0_amb_h21_sum),\n",
    "            ('matrix',       matrix_h21_sum),\n",
    "            ('matrix_pca99', pca_h21_sum)\n",
    "           ],\n",
    "           title='Vector and Tensor Features (sum of the components)',\n",
    "           ylabel='Variable Ranking',\n",
    "           legend='$h_{21}$'\n",
    "          )\n",
    "\n",
    "save_fig('feat_imp_vector_tensor_sum', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better visualisation, we can also plot the sum of scalar, vector and tensor features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of vector and tensor features\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "label_plot(ax,\n",
    "           [('scalars',      scalar_h11_sum),\n",
    "            ('vectors',      vector_h11_sum),\n",
    "            ('matrix',       matrix_h11_sum),\n",
    "            ('matrix_pca99', pca_h11_sum)\n",
    "           ],\n",
    "           title='Sum of the Features (sum of the components)',\n",
    "           ylabel='Variable Ranking',\n",
    "           legend='$h_{11}$'\n",
    "          )\n",
    "label_plot(ax,\n",
    "           [('scalars',      scalar_h21_sum),\n",
    "            ('vectors',      vector_h21_sum),\n",
    "            ('matrix',       matrix_h21_sum),\n",
    "            ('matrix_pca99', pca_h21_sum)\n",
    "           ],\n",
    "           title='Sum of the Features (sum of the components)',\n",
    "           ylabel='Variable Ranking',\n",
    "           legend='$h_{21}$'\n",
    "          )\n",
    "\n",
    "save_fig('feat_imp_sum', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous results, in order to get more valuable results we keep in the engineered set only those variable which where able to reach at least the threshold of 5% in the feature importances. We therefore select:\n",
    "\n",
    "- `num_cp`, `dim_cp`, `matrix_pca99` for $h_{11}$\n",
    "- `num_cp`, `dim_cp`, `dim_h0_amb`, `matrix_pca99` for $h_{21}$\n",
    "\n",
    "We will however keep a copy of the `matrix` (not flattened) to feed the neural networks later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset to file\n",
    "logprint('Saving engineered dataset to file...', logger=logger)\n",
    "df_labs.join(df_feat[['num_cp', 'dim_cp', 'dim_h0_amb', 'matrix_pca99']]).to_hdf(path.join(ROOT_DIR, 'cicy3o_eng.h5.zip'), key='cicy3o_eng', complevel=9, complib='zlib')\n",
    "logprint('Engineered dataset saved to file!', logger=logger)\n",
    "\n",
    "logprint('Saving matrix to file...', logger=logger)\n",
    "df_labs.join(df_feat['matrix']).to_hdf(path.join(ROOT_DIR, 'cicy3o_matrix.h5.zip'), key='cicy3o_matrix', complevel=9, complib='zlib')\n",
    "logprint('Matrix saved to file!', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Analysis\n",
    "\n",
    "We now apply several ML algorithms to the engineered dataset and try to get valuable predictions on the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the dataset\n",
    "DF_ENG_PATH = path.join(ROOT_DIR, 'cicy3o_eng.h5.zst')\n",
    "DF_MAT_PATH = path.join(ROOT_DIR, 'cicy3o_matrix.h5.zst')\n",
    "\n",
    "if path.isfile(DF_ENG_PATH):\n",
    "    logprint('Loading file...', logger=logger)\n",
    "    df      = pd.read_hdf(DF_ENG_PATH)\n",
    "    logprint('File loaded', logger=logger)\n",
    "    \n",
    "    # extract labels and features\n",
    "    df_labs      = df[['h11', 'h21']]\n",
    "    df_feat      = df.drop(labels=['h11', 'h21'], axis=1)\n",
    "    \n",
    "    # divide features according to training sets\n",
    "    logprint('Extracting features from the engineered database...', logger=logger)\n",
    "    num_cp       = ExtractTensor(flatten=True).fit_transform(df_feat['num_cp'])\n",
    "    dim_cp       = ExtractTensor(flatten=True).fit_transform(df_feat['dim_cp'])\n",
    "    features_h11 = np.c_[ExtractTensor(flatten=True).fit_transform(df_feat['num_cp']),\n",
    "                         ExtractTensor(flatten=True).fit_transform(df_feat['dim_cp']),\n",
    "                         ExtractTensor(flatten=True).fit_transform(df_feat['matrix_pca99'])\n",
    "                        ]\n",
    "    features_h21 = np.c_[ExtractTensor(flatten=True).fit_transform(df_feat['num_cp']),\n",
    "                         ExtractTensor(flatten=True).fit_transform(df_feat['dim_cp']),\n",
    "                         ExtractTensor(flatten=True).fit_transform(df_feat['dim_h0_amb']),\n",
    "                         ExtractTensor(flatten=True).fit_transform(df_feat['matrix_pca99'])\n",
    "                        ]\n",
    "    logprint('Features extracted!', logger=logger)\n",
    "else:\n",
    "    logprint('File is not present: cannot read engineered database!', stream='error', logger=logger)\n",
    "    \n",
    "if path.isfile(DF_MAT_PATH):\n",
    "    logprint('Loading file...', logger=logger)\n",
    "    df      = pd.read_hdf(DF_MAT_PATH)\n",
    "    logprint('File loaded', logger=logger)\n",
    "    \n",
    "    # extract labels and features\n",
    "    df_labs      = df[['h11', 'h21']]\n",
    "    df_feat      = df.drop(labels=['h11', 'h21'], axis=1)\n",
    "    \n",
    "    # divide features according to training sets\n",
    "    logprint('Extracting features from the matrix database...', logger=logger)\n",
    "    matrix = ExtractTensor(flatten=True).fit_transform(df_feat['matrix'])\n",
    "    logprint('Features extracted!', logger=logger)\n",
    "else:\n",
    "    logprint('File is not present: cannot read matrix database!', stream='error', logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then build a training and a test sets and define a cross-validation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics         import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt                   import BayesSearchCV\n",
    "from skopt.space             import Categorical, Integer, Real\n",
    "\n",
    "# split into training and test sets\n",
    "features_h11_train, features_h11_test, \\\n",
    "features_h21_train, features_h21_test, \\\n",
    "num_cp_train, num_cp_test, \\\n",
    "dim_cp_train, dim_cp_test, \\\n",
    "matrix_train, matrix_test, \\\n",
    "h11_train, h11_test, \\\n",
    "h21_train, h21_test = train_test_split(features_h11,\n",
    "                                       features_h21,\n",
    "                                       num_cp,\n",
    "                                       dim_cp,\n",
    "                                       matrix,\n",
    "                                       df_labs['h11'].values,\n",
    "                                       df_labs['h21'].values,\n",
    "                                       test_size=0.1,\n",
    "                                       shuffle=True,\n",
    "                                       random_state=RAND\n",
    "                                      )\n",
    "# cross-validation strategy\n",
    "cv     = KFold(n_splits=9, shuffle=False)\n",
    "scorer = lambda x: make_scorer(accuracy_score, greater_is_better=True, rounding=x)\n",
    "\n",
    "# hyperparameter optimization iterations\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each training we will then save the models trained on the feature engineered sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# save the models\n",
    "def save_model(filename, estimator):\n",
    "    \"\"\"Save trained models to file.\n",
    "    \n",
    "    Required arguments:\n",
    "        filename:  the name of the file (w/o extension)\n",
    "        estimator: the model to save\n",
    "    \"\"\"\n",
    "    \n",
    "    MOD_FILE = path.join(MOD_PATH, filename + '.joblib.xz')\n",
    "    \n",
    "    logprint('Saving the estimator to {}.joblib.xz...'.format(filename), logger=logger)\n",
    "    joblib.dump(estimator, MOD_FILE, compress=('xz',9))\n",
    "    logprint('Saved {}.joblib.xz!'.format(filename), logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model    import LinearRegression\n",
    "\n",
    "search_params = {'fit_intercept': [ True, False ],\n",
    "                 'normalize':     [ True, False ]\n",
    "                }\n",
    "rounding = np.floor\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "lin_reg_h11 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(lin_reg_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_reg_h21 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(lin_reg_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "lin_reg_h11 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(lin_reg_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_reg_h21 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(lin_reg_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "lin_reg_h11 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(lin_reg_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_reg_h21 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(lin_reg_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "lin_reg_h11 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(lin_reg_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_reg_h21 = GridSearchCV(LinearRegression(), search_params, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_reg_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(lin_reg_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_reg_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      lin_reg_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      lin_reg_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('lin_reg_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('lin_reg_h11', lin_reg_h11.best_estimator_)\n",
    "save_model('lin_reg_h21', lin_reg_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "search_params = {'alpha':         Real(1e-6, 1e2, base=10, prior='log-uniform'),\n",
    "                 'fit_intercept': Integer(False, True),\n",
    "                 'normalize':     Integer(False, True),\n",
    "                 'positive':      Integer(False, True)\n",
    "                }\n",
    "rounding = np.floor\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "lasso_h11 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(lasso_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lasso_h21 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(lasso_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "lasso_h11 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(lasso_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lasso_h21 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(lasso_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "lasso_h11 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(lasso_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lasso_h21 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(lasso_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "lasso_h11 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(lasso_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lasso_h21 = BayesSearchCV(Lasso(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lasso_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(lasso_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lasso_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      lasso_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      lasso_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('lasso_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('lasso_h11', lasso_h11.best_estimator_)\n",
    "save_model('lasso_h21', lasso_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "search_params = {'alpha':         Real(1e-6, 1e2, base=10, prior='log-uniform'),\n",
    "                 'fit_intercept': Integer(False, True),\n",
    "                 'normalize':     Integer(False, True)\n",
    "                }\n",
    "rounding = np.floor\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "ridge_h11 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(ridge_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "ridge_h21 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(ridge_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "ridge_h11 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(ridge_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "ridge_h21 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(ridge_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "ridge_h11 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(ridge_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "ridge_h21 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(ridge_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "ridge_h11 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(ridge_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "ridge_h21 = BayesSearchCV(Ridge(tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "ridge_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(ridge_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(ridge_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      ridge_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      ridge_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('ridge_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('ridge_h11', ridge_h11.best_estimator_)\n",
    "save_model('ridge_h21', ridge_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "search_params = {'alpha':         Real(1e-6, 1e2, base=10, prior='log-uniform'),\n",
    "                 'fit_intercept': Integer(False, True),\n",
    "                 'normalize':     Integer(False, True)\n",
    "                }\n",
    "rounding = np.floor\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "el_net_h11 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(el_net_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "el_net_h21 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(el_net_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "el_net_h11 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(el_net_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "el_net_h21 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(el_net_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "el_net_h11 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(el_net_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "el_net_h21 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(el_net_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "el_net_h11 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(el_net_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "el_net_h21 = BayesSearchCV(ElasticNet(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "el_net_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(el_net_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(el_net_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      el_net_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      el_net_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('el_net_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('el_net_h11', el_net_h11.best_estimator_)\n",
    "save_model('el_net_h21', el_net_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "search_params = {'C':                 Real(1e-4, 1e4, base=10, prior='log-uniform'),\n",
    "                 'intercept_scaling': Real(1e-2, 1e2, base=10, prior='log-uniform'),\n",
    "                 'fit_intercept':     Integer(False, True),\n",
    "                 'loss':              Categorical(['epsilon_insensitive', 'squared_epsilon_insensitive'])\n",
    "                }\n",
    "rounding = np.floor\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "lin_svr_h11 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(lin_svr_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_svr_h21 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(lin_svr_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "lin_svr_h11 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(lin_svr_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_svr_h21 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(lin_svr_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "lin_svr_h11 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(lin_svr_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_svr_h21 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(lin_svr_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "lin_svr_h11 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(lin_svr_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "lin_svr_h21 = BayesSearchCV(LinearSVR(max_iter=15000, tol=0.001, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "lin_svr_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(lin_svr_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(lin_svr_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      lin_svr_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      lin_svr_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('lin_svr_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('lin_svr_h11', lin_svr_h11.best_estimator_)\n",
    "save_model('lin_svr_h21', lin_svr_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "search_params = {'C':         Real(1e-4, 1e4, base=10, prior='log-uniform'),\n",
    "                 'gamma':     Real(1e-6, 1e2, base=10, prior='log-uniform'),\n",
    "                 'epsilon':   Real(1e-5, 1e1, base=10, prior='log-uniform'),\n",
    "                 'shrinking': Integer(False, True)\n",
    "                }\n",
    "rounding = np.rint\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "svr_rbf_h11 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(svr_rbf_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "svr_rbf_h21 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(svr_rbf_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "svr_rbf_h11 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(svr_rbf_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "svr_rbf_h21 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(svr_rbf_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "svr_rbf_h11 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(svr_rbf_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "svr_rbf_h21 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(svr_rbf_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "svr_rbf_h11 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(svr_rbf_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "svr_rbf_h21 = BayesSearchCV(SVR(kernel='rbf', tol=0.001), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "svr_rbf_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(svr_rbf_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(svr_rbf_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      svr_rbf_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      svr_rbf_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('svr_rbf_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('svr_rbf_h11', svr_rbf_h11.best_estimator_)\n",
    "save_model('svr_rbf_h21', svr_rbf_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "search_params = {'n_estimators':      Integer(2, 75, prior='uniform'),\n",
    "                 'criterion':         Categorical(['friedman_mse', 'mae']),\n",
    "                 'min_samples_split': Integer(2, 10, prior='uniform'),\n",
    "                 'min_samples_leaf':  Integer(1, 50, prior='uniform'),\n",
    "                 'max_depth':         Integer(2, 20, prior='uniform')\n",
    "                }\n",
    "rounding = np.floor\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "rnd_for_h11 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(rnd_for_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "rnd_for_h21 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(rnd_for_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "rnd_for_h11 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(rnd_for_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "rnd_for_h21 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(rnd_for_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "rnd_for_h11 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(rnd_for_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "rnd_for_h21 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(rnd_for_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "rnd_for_h11 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(rnd_for_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "rnd_for_h21 = BayesSearchCV(RandomForestRegressor(n_jobs=-1, random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=1, refit=True, cv=cv)\n",
    "rnd_for_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(rnd_for_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(rnd_for_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      rnd_for_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      rnd_for_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('rnd_for_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('rnd_for_h11', rnd_for_h11.best_estimator_)\n",
    "save_model('rnd_for_h21', rnd_for_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "search_params = {'loss':              Categorical(['ls', 'lad']),\n",
    "                 'learning_rate':     Real(1e-4, 1e1, base=10, prior='log-uniform'),\n",
    "                 'n_estimators':      Integer(100, 500, prior='uniform'),\n",
    "                 'subsample':         Real(0.6, 1.0, prior='uniform'),\n",
    "                 'criterion':         Categorical(['friedman_mse', 'mae']),\n",
    "                 'min_samples_split': Integer(2, 10, prior='uniform'),\n",
    "                 'min_samples_leaf':  Integer(1, 50, prior='uniform'),\n",
    "                 'max_depth':         Integer(2, 20, prior='uniform')\n",
    "                }\n",
    "rounding = np.floor\n",
    "\n",
    "# compute matrix baseline\n",
    "logprint('Fitting the matrix baseline...', logger=logger)\n",
    "grd_boost_h11 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h11.fit(matrix_train, h11_train)\n",
    "gridcv_score(grd_boost_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h11, matrix_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "grd_boost_h21 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h21.fit(matrix_train, h21_train)\n",
    "gridcv_score(grd_boost_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h21, matrix_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute num_cp baseline\n",
    "logprint('Fitting the num_cp baseline...', logger=logger)\n",
    "grd_boost_h11 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h11.fit(np.reshape(num_cp_train, (-1,1)), h11_train)\n",
    "gridcv_score(grd_boost_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h11, np.reshape(num_cp_test, (-1,1)), h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "grd_boost_h21 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h21.fit(np.reshape(num_cp_train, (-1,1)), h21_train)\n",
    "gridcv_score(grd_boost_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h21, np.reshape(num_cp_test, (-1,1)), h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute dim_cp baseline\n",
    "logprint('Fitting the dim_cp baseline...', logger=logger)\n",
    "grd_boost_h11 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h11.fit(dim_cp_train, h11_train)\n",
    "gridcv_score(grd_boost_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h11, dim_cp_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "grd_boost_h21 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h21.fit(dim_cp_train, h21_train)\n",
    "gridcv_score(grd_boost_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h21, dim_cp_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# compute feature engineered dataset\n",
    "logprint('Fitting the feature engineered dataset...', logger=logger)\n",
    "grd_boost_h11 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h11.fit(features_h11_train, h11_train)\n",
    "gridcv_score(grd_boost_h11, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h11, features_h11_test, h11_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "grd_boost_h21 = BayesSearchCV(GradientBoostingRegressor(random_state=RAND), search_params, n_iter=n_iter, scoring=scorer(rounding), n_jobs=-1, refit=True, cv=cv)\n",
    "grd_boost_h21.fit(features_h21_train, h21_train)\n",
    "gridcv_score(grd_boost_h21, rounding=rounding, logger=logger)\n",
    "prediction_score(grd_boost_h21, features_h21_test, h21_test, use_best_estimator=True, rounding=rounding, logger=logger)\n",
    "\n",
    "# plot the error difference of the feature engineered dataset\n",
    "logprint('Plotting error distribution...', logger=logger)\n",
    "xplots  = 1\n",
    "yplots  = 1\n",
    "fig, ax = plt.subplots(yplots, xplots, figsize=(xplots*mpl_width, yplots*mpl_height))\n",
    "fig.tight_layout()\n",
    "\n",
    "count_plot(ax,\n",
    "           error_diff(h11_test,\n",
    "                      grd_boost_h11.best_estimator_.predict(features_h11_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{11}$')\n",
    "count_plot(ax,\n",
    "           error_diff(h21_test,\n",
    "                      grd_boost_h21.best_estimator_.predict(features_h21_test),\n",
    "                      rounding=rounding),\n",
    "           title='Error distribution on the test set',\n",
    "           xlabel='Difference from real value',\n",
    "           legend='$h_{21}$')\n",
    "\n",
    "save_fig('grd_boost_error_eng', logger=logger)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Saving the feature engineered models\n",
    "save_model('grd_boost_h11', grd_boost_h11.best_estimator_)\n",
    "save_model('grd_boost_h21', grd_boost_h21.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
